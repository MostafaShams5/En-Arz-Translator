{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Shams\n",
    "\n",
    "**Description:**\n",
    "This is the core training script. I am building a bilingual Encoder-Decoder model from scratch. I am initializing a fresh neural network with random weights and teaching it English and Egyptian Arabic from zero.\n",
    "\n",
    "**Architecture & Strategy:**\n",
    "\n",
    "1.  **Encoder-Decoder Architecture:**\n",
    "    I chose this because I need the model to understand the full context (Encoder) and then generate text step-by-step (Decoder). It has 8 Encoder layers and 8 Decoder layers with a hidden size of 384. It's smaller than the standard base models, but it fits my data and hardware better.\n",
    "\n",
    "2.  **Weighted Data Sampling:**\n",
    "    Not all my data is equal. I have some really clean English text and some high-quality Arabic, but I also have some \"meh\" scraped data. If I just mixed them equally, the model would learn bad habits.\n",
    "    *   **The Big English:** 54% (The backbone of grammar).\n",
    "    *   **High-Tier Arabic:** 24% (The target language).\n",
    "    *   **Lower Tiers:** Sampled much less (just for variety).\n",
    "    I use `interleave_datasets` to mix these streams based on these exact percentages.\n",
    "\n",
    "3.  **The RMSNorm Fix (Crucial):**\n",
    "    In my previous attempts, the training kept crashing. The loss would go to `NaN` (Not a Number) because of \"gradient explosions.\" The standard `LayerNorm` used in this architecture was too unstable for half-precision (FP16) training.\n",
    "    *   **Solution:** I wrote a custom script to physically swap out every `LayerNorm` layer in the model and replace it with `RMSNorm` (Root Mean Square Normalization). This is much more stable and fixed the explosion issue.\n",
    "\n",
    "4.  **Manual Training Loop:**\n",
    "    I am not using the standard `.train()` method. I wrote a manual loop to control exactly how the state is saved and loaded. This allows me to save the exact position of the data iterator so I don't restart the dataset from row 0 every time I pause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Installs & Imports\n",
    "Getting the environment ready. I need `accelerate` to handle the GPU hardware and `datasets` to stream the massive text files without crashing RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate tensorboard bitsandbytes\n",
    "\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from datasets import load_dataset, interleave_datasets, IterableDataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BartConfig,\n",
    "    BartForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    HfArgumentParser,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    PreTrainedTokenizerBase,\n",
    "    TrainerCallback,\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    "    get_scheduler,\n",
    ")\n",
    "from transformers.utils import PaddingStrategy\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuration\n",
    "This is where I define the weighted sampling strategy. You can see in `DataArgs` how I assign probabilities to each dataset split. \n",
    "\n",
    "I also set `max_grad_norm=1.5` here. This clips the gradients if they get too big, which acts as a second safety net alongside the RMSNorm fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "from transformers import TrainingArguments, HfArgumentParser\n",
    "from accelerate import Accelerator\n",
    "import os\n",
    "import sys\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    model_output_dir: str = \"/kaggle/working/bart-arz-en-pretrained\"\n",
    "    tokenizer_path: str = \"/kaggle/input/bpescratch/ARZ-EN-BART-Tokenizer/\"\n",
    "    dataset_repo_id: str = \"Shams03/Tokenized-ARZ-EN-BART\"\n",
    "    cache_dir: str = \"/kaggle/working/cache\"\n",
    "\n",
    "    # Architecture specs: 8 layers each, 384 hidden dim\n",
    "    encoder_layers: int = 8\n",
    "    decoder_layers: int = 8\n",
    "    d_model: int = 384\n",
    "    num_heads: int = 12\n",
    "    ffn_dim: int = 1152\n",
    "\n",
    "@dataclass\n",
    "class DataArgs:\n",
    "    # --- WEIGHTED SAMPLING STRATEGY ---\n",
    "    # I give higher probability to clean English and High-Tier Arabic.\n",
    "    # This forces the model to learn from good data more often.\n",
    "    sampling_probabilities: Dict[str, float] = field(default_factory=lambda: {\n",
    "        \"TheBigEN\": 0.54, \n",
    "        \"A_Tier_ARZ\": 0.24, \n",
    "        \"S_Tier_ARZ\": 0.09, \n",
    "        \"B_Tier_ARZ\": 0.07,\n",
    "        \"parallel_EN\": 0.04, \n",
    "        \"LparallelEN\": 0.02, \n",
    "        \"parallel_ARZ\": 0.04, \n",
    "        \"Lparallel_ARZ\": 0.02\n",
    "    })\n",
    "    streaming: bool = True\n",
    "    max_seq_length: int = 256\n",
    "\n",
    "@dataclass\n",
    "class PretrainArgs(TrainingArguments):\n",
    "    output_dir: str = \"/kaggle/working/bart-arz-en-pretrained\"\n",
    "    max_steps: int = 320000\n",
    "    per_device_train_batch_size: int = 15\n",
    "    gradient_accumulation_steps: int = 10\n",
    "\n",
    "    learning_rate: float = 1e-5\n",
    "    warmup_steps: int = 1000\n",
    "    weight_decay: float = 0.01\n",
    "    adam_beta1: float = 0.9\n",
    "    adam_beta2: float = 0.98\n",
    "    lr_scheduler_type: str = 'linear'\n",
    "    num_train_epochs: float = 100.0\n",
    "\n",
    "    logging_strategy: str = \"steps\"\n",
    "    logging_steps: int = 100\n",
    "    save_strategy: str = \"steps\"\n",
    "    save_steps: int = 1000\n",
    "    save_total_limit: int = 1\n",
    "    fp16: bool = True\n",
    "    dataloader_num_workers: int = 1\n",
    "    seed: int = 42\n",
    "    report_to: str = \"tensorboard\"\n",
    "    resume_from_checkpoint: bool = True\n",
    "\n",
    "    # Safety nets for gradient explosion\n",
    "    adam_epsilon=1e-6, \n",
    "    max_grad_norm=1.5 \n",
    "    torch_compile: bool = True\n",
    "    gradient_checkpointing: bool = False\n",
    "    mlm_probability: float = 0.15\n",
    "\n",
    "parser = HfArgumentParser((ModelArgs, DataArgs, PretrainArgs))\n",
    "\n",
    "if \"ipykernel\" in sys.modules:\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses(args=[])\n",
    "else:\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "os.makedirs(model_args.model_output_dir, exist_ok=True)\n",
    "os.makedirs(model_args.cache_dir, exist_ok=True)\n",
    "\n",
    "print(\"Model Args:\")\n",
    "print(model_args)\n",
    "print(\"\\nData Args:\")\n",
    "print(data_args)\n",
    "print(\"\\nTraining Args:\")\n",
    "print(f\"  Output: {training_args.output_dir}\")\n",
    "print(f\"  Steps: {training_args.max_steps}\")\n",
    "print(f\"  LR: {training_args.learning_rate}\")\n",
    "print(f\"  FP16: {training_args.fp16}\")\n",
    "\n",
    "accelerator = Accelerator()\n",
    "print(\"\\nHardware:\")\n",
    "print(f\"GPUs: {accelerator.num_processes}\")\n",
    "print(f\"Batch Size: {training_args.per_device_train_batch_size}\")\n",
    "\n",
    "num_gpus = max(1, accelerator.num_processes)\n",
    "eff_batch_size = training_args.per_device_train_batch_size * num_gpus * training_args.gradient_accumulation_steps\n",
    "print(f\"Effective Batch: {eff_batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Tokenizer\n",
    "Loading the custom BPE tokenizer. I have to make sure the special tokens are mapped correctly. The Encoder-Decoder model relies on `<s>` (start) and `</s>` (end) to know when a sentence begins and ends. If these IDs are wrong, the model learns nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading tokenizer: {model_args.tokenizer_path}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_path,\n",
    "    cache_dir=model_args.cache_dir\n",
    ")\n",
    "\n",
    "print(\"Loaded.\")\n",
    "print(f\"Type: {type(tokenizer)}\")\n",
    "print(f\"Vocab: {tokenizer.vocab_size}\")\n",
    "assert tokenizer.vocab_size == 90000, \"Wrong vocab size!\"\n",
    "\n",
    "# Checking special tokens. This is critical for the model structure.\n",
    "print(f\"PAD: {tokenizer.pad_token} ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"UNK: {tokenizer.unk_token} ID: {tokenizer.unk_token_id}\")\n",
    "print(f\"MASK: {tokenizer.mask_token} ID: {tokenizer.mask_token_id}\")\n",
    "print(f\"BOS: {tokenizer.bos_token} ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"EOS: {tokenizer.eos_token} ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"CLS: {tokenizer.cls_token} ID: {tokenizer.cls_token_id}\")\n",
    "print(f\"SEP: {tokenizer.sep_token} ID: {tokenizer.sep_token_id}\")\n",
    "\n",
    "# Assertions to ensure compatibility with the model config\n",
    "assert tokenizer.bos_token_id == 3\n",
    "assert tokenizer.eos_token_id == 4\n",
    "assert tokenizer.cls_token_id == tokenizer.bos_token_id\n",
    "assert tokenizer.sep_token_id == tokenizer.eos_token_id\n",
    "assert tokenizer.add_prefix_space == True\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "bos_token_id = tokenizer.bos_token_id\n",
    "eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Loading & Interleaving\n",
    "Here I implement the weighted sampling. I load each split separately. Then I use `interleave_datasets` with the `probabilities` list I defined in the config. \n",
    "\n",
    "This creates a single stream of data where 54% of rows are from the big English dataset, 24% from the best Arabic, and so on. I also add a `shuffle` buffer to mix them up locally so the model doesn't see 1000 English sentences in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading data from: {model_args.dataset_repo_id}\")\n",
    "\n",
    "available_splits = [\n",
    "    'A_Tier_ARZ', 'B_Tier_ARZ', 'Lparallel_ARZ', 'LparallelEN',\n",
    "    'S_Tier_ARZ', 'TheBigEN', 'parallel_ARZ', 'parallel_EN'\n",
    "]\n",
    "sampling_probabilities = data_args.sampling_probabilities\n",
    "split_datasets = []\n",
    "split_probs = []\n",
    "total_prob = 0\n",
    "\n",
    "print(\"Loading splits...\")\n",
    "for split_name in available_splits:\n",
    "    if split_name not in sampling_probabilities:\n",
    "        continue\n",
    "    prob = sampling_probabilities[split_name]\n",
    "    print(f\"- Loading {split_name} (Prob: {prob})\")\n",
    "    try:\n",
    "        ds = load_dataset(\n",
    "            model_args.dataset_repo_id,\n",
    "            split=split_name,\n",
    "            streaming=data_args.streaming,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "        )\n",
    "        split_datasets.append(ds)\n",
    "        split_probs.append(prob)\n",
    "        total_prob += prob\n",
    "    except Exception as e:\n",
    "        print(f\"  Failed to load {split_name}: {e}\")\n",
    "        raise RuntimeError(f\"Missing dataset: {split_name}\")\n",
    "\n",
    "if not split_datasets: raise RuntimeError(\"No data loaded.\")\n",
    "\n",
    "# Normalizing probs to sum exactly to 1.0\n",
    "if not (0.99 < total_prob < 1.01):\n",
    "    norm_factor = 1.0 / total_prob\n",
    "    split_probs = [p * norm_factor for p in split_probs]\n",
    "\n",
    "print(\"Mixing datasets...\")\n",
    "interleaved_dataset = interleave_datasets(\n",
    "    split_datasets,\n",
    "    probabilities=split_probs,\n",
    "    seed=training_args.seed,\n",
    "    stopping_strategy=\"all_exhausted\"\n",
    ")\n",
    "\n",
    "# Shuffle buffer to ensure randomness in the stream\n",
    "shuffle_buffer_size = 100000\n",
    "print(f\"Shuffling with buffer {shuffle_buffer_size}...\")\n",
    "combined_dataset = interleaved_dataset.shuffle(\n",
    "    seed=training_args.seed,\n",
    "    buffer_size=shuffle_buffer_size\n",
    ")\n",
    "print(\"Data ready.\")\n",
    "\n",
    "if data_args.streaming:\n",
    "    try:\n",
    "        first_example = next(iter(combined_dataset))\n",
    "        print(f\"Check first example: {first_example.keys()}\")\n",
    "    except Exception as e: \n",
    "        print(f\"Error checking data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Data Inspection\n",
    "Just a quick sanity check to see the raw IDs. I want to make sure I'm getting actual data and not empty rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples_to_show = 5\n",
    "actual_count = 0\n",
    "\n",
    "try:\n",
    "    dataset_iterator = iter(combined_dataset)\n",
    "    for i in range(num_examples_to_show):\n",
    "        example = next(dataset_iterator)\n",
    "        input_ids = example['input_ids']\n",
    "        clean_decoded = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"  IDs: {input_ids[:10]}...\")\n",
    "        print(f\"  Text: '{clean_decoded}'\")\n",
    "        actual_count += 1\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Collator (The Masking Logic)\n",
    "This is the pre-training objective. The model reads a sentence with \"holes\" in it and has to guess what's missing.\n",
    "\n",
    "1.  **Padding:** Pads the batch to the same length.\n",
    "2.  **Special Tokens:** It adds `<s>` at the start and `</s>` at the end.\n",
    "3.  **Masking:** It randomly selects 15% of the tokens to hide.\n",
    "4.  **Protection:** It specifically checks `special_tokens_mask` to make sure we NEVER mask the `<s>`, `</s>`, or `<pad>` tokens. If we masked those, the model would lose track of where sentences begin/end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForBartPretraining:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    mlm_probability: float = 0.15\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    max_seq_length: Optional[int] = None\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, List[int]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch_input_ids = [e['input_ids'] for e in examples]\n",
    "\n",
    "        processed_batch = {\n",
    "            \"input_ids\": [],\n",
    "            \"labels\": []\n",
    "        }\n",
    "\n",
    "        max_len_no_special = self.max_seq_length - 2 if self.max_seq_length else None\n",
    "\n",
    "        for ids in batch_input_ids:\n",
    "            if max_len_no_special:\n",
    "                ids = ids[:max_len_no_special]\n",
    "            # manually adding BOS and EOS\n",
    "            processed_batch[\"input_ids\"].append([self.tokenizer.bos_token_id] + ids + [self.tokenizer.eos_token_id])\n",
    "            processed_batch[\"labels\"].append([self.tokenizer.bos_token_id] + ids + [self.tokenizer.eos_token_id])\n",
    "\n",
    "        # pad labels first\n",
    "        labels_batch = self.tokenizer.pad(\n",
    "            {\"input_ids\": processed_batch[\"labels\"]},\n",
    "            padding='longest',\n",
    "            max_length=self.max_seq_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = labels_batch[\"input_ids\"]\n",
    "\n",
    "        # pad inputs\n",
    "        inputs_batch = self.tokenizer.pad(\n",
    "            {\"input_ids\": processed_batch[\"input_ids\"]},\n",
    "            padding='longest',\n",
    "            max_length=self.max_seq_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = inputs_batch[\"input_ids\"]\n",
    "        attention_mask = inputs_batch[\"attention_mask\"]\n",
    "\n",
    "        # apply the masking logic\n",
    "        inputs, labels = self.mask_tokens(input_ids, labels)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "    def mask_tokens(self, inputs: torch.Tensor, labels: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        masked_inputs = inputs.clone()\n",
    "        mlm_labels = labels.clone()\n\n",
    "        probability_matrix = torch.full(mlm_labels.shape, self.mlm_probability)\n",
    "\n",
    "        # prevent masking special tokens. THIS IS IMPORTANT.\n",
    "        padding_mask = mlm_labels.eq(self.tokenizer.pad_token_id)\n",
    "        bos_mask = mlm_labels.eq(self.tokenizer.bos_token_id)\n",
    "        eos_mask = mlm_labels.eq(self.tokenizer.eos_token_id)\n",
    "\n",
    "        special_tokens_mask = padding_mask | bos_mask | eos_mask\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "\n",
    "        # set unmasked labels to -100 so loss ignores them\n",
    "        mlm_labels[~masked_indices] = -100\n",
    "\n",
    "        # 80% replace with [MASK]\n",
    "        indices_replaced = torch.bernoulli(torch.full(mlm_labels.shape, 0.8)).bool() & masked_indices\n        masked_inputs[indices_replaced] = self.tokenizer.mask_token_id\n",
    "\n",
    "        # 10% replace with random word\n",
    "        indices_random = torch.bernoulli(torch.full(mlm_labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(low=0, high=self.tokenizer.vocab_size, size=mlm_labels.shape, dtype=torch.long)\n",
    "        \n",
    "        # safety check for random words (don't pick special tokens)\n",
    "        for special_id in [self.tokenizer.pad_token_id, self.tokenizer.bos_token_id, self.tokenizer.eos_token_id]:\n",
    "            random_words[random_words == special_id] = self.tokenizer.mask_token_id\n",
    "\n",
    "        masked_inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        return masked_inputs, mlm_labels\n",
    "\n",
    "data_collator = DataCollatorForBartPretraining(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=training_args.mlm_probability,\n",
    "    max_seq_length=data_args.max_seq_length,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "print(\"Collator ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Initialization & RMSNorm Fix\n",
    "This is the most critical part of the architecture setup. \n",
    "\n",
    "1.  **Initialization:** I use `init_std=0.02` and a custom `init_weights` function. Randomly initialized Transformers are very fragile. If the weights are too big, the math breaks immediately.\n",
    "2.  **RMSNorm Replacement:** Standard `LayerNorm` calculates `(x - mean) / std`. In deep networks with mixed precision (FP16), `std` can get tiny, causing division by zero or huge numbers. `RMSNorm` removes the `mean` calculation and just scales by the root mean square. It is mathematically safer. I wrote a function `replace_layernorm_with_rmsnorm` that hunts down every `LayerNorm` in the model and swaps it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Encoder-Decoder Model...\")\n",
    "\n",
    "config = BartConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    pad_token_id=pad_token_id,\n",
    "    bos_token_id=bos_token_id,\n",
    "    eos_token_id=eos_token_id,\n",
    "    encoder_layers=model_args.encoder_layers,\n",
    "    decoder_layers=model_args.decoder_layers,\n",
    "    d_model=model_args.d_model,\n",
    "    encoder_attention_heads=model_args.num_heads,\n",
    "    decoder_attention_heads=model_args.num_heads,\n",
    "    encoder_ffn_dim=model_args.ffn_dim,\n",
    "    decoder_ffn_dim=model_args.ffn_dim,\n",
    "    activation_function=\"gelu\",\n",
    "    dropout=0.1,\n",
    "    attention_dropout=0.1,\n",
    "    activation_dropout=0.1,\n",
    "    scale_embedding=True,\n",
    "    init_std=0.02\n",
    ")\n",
    "\n",
    "model = BartForConditionalGeneration(config=config)\n",
    "\n",
    "# --- THE RMSNORM FIX ---\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return x / rms * self.scale\n",
    "\n",
    "def replace_layernorm_with_rmsnorm(module: nn.Module):\n",
    "    # Recursive function to find and kill LayerNorm\n",
    "    for name, child in list(module.named_children()):\n",
    "        if isinstance(child, nn.LayerNorm):\n",
    "            dim = child.normalized_shape[0] if isinstance(child.normalized_shape, (tuple, list)) else child.normalized_shape\n",
    "            rms = RMSNorm(dim=dim, eps=1e-6)\n",
    "            # try to preserve weights if they exist\n",
    "            if getattr(child, \"weight\", None) is not None:\n                try:\n                    with torch.no_grad():\n                        rms.scale.copy_(child.weight)\n                except Exception:\n                    pass\n            setattr(module, name, rms)\n        else:\n            replace_layernorm_with_rmsnorm(child)\n\n# --- Robust Init ---\n# Standard init can be unstable. This ensures weights start small and safe.\ndef init_weights(module):\n    if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):\n        try:\n            fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(module.weight)\n            std = (fan_in ** -0.5) if fan_in > 0 else 0.02\n        except Exception:\n            std = 0.02\n        nn.init.normal_(module.weight, mean=0.0, std=std)\n        if getattr(module, \"bias\", None) is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Embedding):\n        d = module.weight.size(1)\n        std = d ** -0.5\n        nn.init.normal_(module.weight, mean=0.0, std=std)\n    elif isinstance(module, RMSNorm):\n        nn.init.ones_(module.scale)\n\nprint(\"Swapping LayerNorm for RMSNorm...\")\nreplace_layernorm_with_rmsnorm(model)\n\nprint(\"Applying custom initialization...\")\nmodel.apply(init_weights)\n\nmodel_size = sum(t.numel() for t in model.parameters())\nprint(f\"Model Ready. Parameters: {model_size / 1_000_000:.1f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Accelerator Setup\n",
    "Preparing the `Accelerator`. This library handles the heavy lifting of moving tensors to the GPU and managing mixed precision (FP16). I initialize the `Trainer` here, but primarily to use its underlying utilities, not its main loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "try:\n",
    "    mixed_precision_arg = \"fp16\" if getattr(training_args, \"fp16\", False) else \"no\"\n",
    "    accelerator = Accelerator(mixed_precision=mixed_precision_arg)\n",
    "    print(f\"Accelerator: {mixed_precision_arg}\")\n",
    "except TypeError:\n",
    "    accelerator = Accelerator()\n",
    "    print(\"Accelerator: Default\")\n",
    "\n",
    "device = accelerator.device\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "print(\"Initializing Trainer wrapper...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(\"Trainer ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Collator Check\n",
    "A final check before the loop. I grab one batch and check for `NaN` values. If the inputs are corrupted here, the whole training is doomed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking batch...\")\n",
    "try:\n",
    "    dataloader = trainer.get_train_dataloader()\n    batch = next(iter(dataloader))\n    \n    input_ids_ok = not torch.isnan(batch['input_ids']).any()\n    labels_ok = not torch.isnan(batch['labels']).any()\n    \n    print(f\"  Inputs OK: {input_ids_ok}\")\n    print(f\"  Labels OK: {labels_ok}\")\n\n    if not (input_ids_ok and labels_ok):\n        print(\"  WARNING: Bad batch detected.\")\n\nexcept Exception as e:\n    print(f\"  Error checking batch: {e}\")\n\nif 'dataloader' in locals():\n   del dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. The Manual Training Loop\n",
    "I am writing my own training loop instead of using `trainer.train()`. Why? **Control.**\n",
    "\n",
    "1.  **State Loading:** Standard trainers often restart the data iterator from the beginning when resuming. Since I have massive datasets, I need to save the `dataloader_state.pt` and `dataset_state.pt` so I can resume exactly where I left off (skipping millions of rows instantly).\n",
    "2.  **Gradient Scaling:** I manually control the `GradScaler` for FP16 to prevent underflow.\n",
    "3.  **Diagnostics:** Every 100 steps, I run a small evaluation on specific sentences (like \"Today `<mask>` a beautiful day\") to see if the model is learning grammar live.\n",
    "4.  **Checkpointing:** I save everything—Optimizer, Scheduler, Model, and Data state—into one folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "import math\n",
    "import shutil\n",
    "import re\n",
    "import json\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import torch \n",
    "\n",
    "print(\"Starting Training Loop.\")\n",
    "\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate,\n",
    "                  betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
    "                  eps=getattr(training_args, \"adam_epsilon\", 1e-6),\n",
    "                  weight_decay=training_args.weight_decay)\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    name=training_args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(getattr(training_args, \"warmup_steps\", 0)),\n",
    "    num_training_steps=training_args.max_steps,\n",
    ")\n",
    "\n",
    "use_native_amp = bool(getattr(training_args, \"fp16\", False) and torch.cuda.is_available())\n",
    "scaler = GradScaler() if use_native_amp else None\n",
    "\n",
    "model = accelerator.prepare(model)\n",
    "\n",
    "# Diagnostic sentences to watch learning progress\n",
    "mask = tokenizer.mask_token \n",
    "test_sentences = [\n",
    "    f\"الجو برد اوي النهاردة، أنا هقعد {mask} البيت.\",\n",
    "    f\"He bought bread {mask} milk from the store.\",\n",
    "    f\"Today {mask} a beautiful day.\",\n",
    "    f\"أنا صحيت الصبح و أول حاجة عملتها إني {mask}{mask} كوباية شاي.\",\n",
    "]\n",
    "top_k = 5\n",
    "\n",
    "# --- RESUME LOGIC ---\n",
    "global_step = 0\n",
    "start_micro_step = 0 \n",
    "did_load_iterator_state = False\n",
    "\n",
    "# If I have a checkpoint path, I use it here\n",
    "MANUAL_CHECKPOINT_PATH = \"/kaggle/input/arz-en-bart/bart-arz-en-pretrained/checkpoint-214000\"\n",
    "\n",
    "resume_ckpt = None\n",
    "if training_args.resume_from_checkpoint and MANUAL_CHECKPOINT_PATH and os.path.isdir(MANUAL_CHECKPOINT_PATH):\n",
    "    resume_ckpt = MANUAL_CHECKPOINT_PATH\n",
    "\n",
    "if resume_ckpt:\n",
    "    print(f\"Resuming from: {resume_ckpt}\")\n",
    "    try:\n",
    "        accelerator.load_state(resume_ckpt)\n",
    "        print(\"Model state loaded.\")\n",
    "    except Exception as e: print(f\"Warn: Model load failed: {e}\")\n",
    "        \n",
    "    # Loading optimizer/scheduler states\n",
    "    opt_path = os.path.join(resume_ckpt, \"optimizer.pt\")\n",
    "    if os.path.exists(opt_path):\n",
    "        optimizer.load_state_dict(torch.load(opt_path, map_location=accelerator.device))\n",
    "\n",
    "    sch_path = os.path.join(resume_ckpt, \"scheduler.pt\")\n",
    "    if os.path.exists(sch_path):\n",
    "        scheduler.load_state_dict(torch.load(sch_path, map_location=\"cpu\"))\n",
    "\n",
    "    if scaler and os.path.exists(os.path.join(resume_ckpt, \"scaler.pt\")):\n",
    "        scaler.load_state_dict(torch.load(os.path.join(resume_ckpt, \"scaler.pt\"), map_location=\"cpu\"))\n",
    "\n",
    "    m = re.search(r\"-(\\d+)$\", resume_ckpt)\n",
    "    if m:\n",
    "        global_step = int(m.group(1))\n",
    "        start_micro_step = global_step * training_args.gradient_accumulation_steps\n",
    "\n",
    "    # Loading dataset state. This is key to not restarting data from row 0.\n",
    "    ds_state_path = os.path.join(resume_ckpt, \"dataset_state.pt\")\n",
    "    if os.path.exists(ds_state_path):\n",
    "        combined_dataset.load_state_dict(torch.load(ds_state_path, map_location=\"cpu\"))\n",
    "        train_dataloader = trainer.get_train_dataloader()\n",
    "        \n",
    "        dl_state_path = os.path.join(resume_ckpt, \"dataloader_state.pt\")\n",
    "        if os.path.exists(dl_state_path):\n",
    "            train_dataloader.load_state_dict(torch.load(dl_state_path, map_location=\"cpu\"))\n",
    "            did_load_iterator_state = True\n",
    "            print(\"Dataset iterator restored.\")\n",
    "\n",
    "train_dataloader = accelerator.prepare(train_dataloader)\n",
    "\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "grad_accum = training_args.gradient_accumulation_steps\n",
    "max_steps = training_args.max_steps\n",
    "train_dataloader_iter = iter(train_dataloader)\n",
    "\n",
    "# Fast forward if we failed to load iterator state (fallback)\n",
    "if start_micro_step > 0 and not did_load_iterator_state:\n",
    "    print(f\"Fast-forwarding {start_micro_step} steps...\")\n",
    "    for _ in range(start_micro_step):\n",
    "        next(train_dataloader_iter)\n",
    "    print(\"Done.\")\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "for micro_step, batch in enumerate(train_dataloader_iter, start=start_micro_step):\n",
    "\n",
    "    # Calculate where we are globally\n",
    "    if (micro_step + 1) % grad_accum == 0:\n",
    "        prospective_global = (micro_step + 1) // grad_accum\n",
    "    else:\n",
    "        prospective_global = global_step \n",
    "\n",
    "    if prospective_global >= max_steps:\n",
    "        break\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch.get(\"attention_mask\", None)\n",
    "    labels = batch.get(\"labels\", None)\n",
    "\n",
    "    # Forward Pass\n",
    "    if scaler is not None:\n",
    "        with autocast():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss / max(1, grad_accum)\n",
    "        scaler.scale(loss).backward()\n",
    "    else:\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss / max(1, grad_accum)\n",
    "        loss.backward()\n",
    "\n",
    "    # Optimizer Step (only after accumulation)\n",
    "    if (micro_step + 1) % grad_accum == 0:\n",
    "        global_step = (micro_step + 1) // grad_accum\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), training_args.max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), training_args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Logging\n",
    "        if global_step % training_args.logging_steps == 0 or global_step == 1:\n",
    "            elapsed = time.time() - start_time\n",
    "            reported_loss = (loss.item() * grad_accum)\n",
    "            current_lr = scheduler.get_last_lr()[0] \n",
    "            print(f\"\\nStep {global_step} | Loss: {reported_loss:.4f} | LR: {current_lr:.2e} | Time: {elapsed:.0f}s\")\n",
    "\n",
    "            # Live Evaluation\n",
    "            model.eval()\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    for sent in test_sentences:\n",
    "                        sent_fmt = sent.replace(\"<mask>\", tokenizer.mask_token)\n",
    "                        enc = tokenizer(sent_fmt, return_tensors=\"pt\", truncation=True, max_length=128).to(accelerator.device)\n",
    "                        outputs = model(**enc)\n",
    "                        logits = outputs.logits\n",
    "                        mask_pos = (enc.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=False)\n",
    "                        if len(mask_pos) > 0:\n",
    "                            print(f\"  Input: {sent}\")\n",
    "                            for m in mask_pos:\n",
    "                                b, p = m[0], m[1]\n",
    "                                vals, ids = torch.topk(logits[b, p], k=top_k)\n",
    "                                toks = tokenizer.convert_ids_to_tokens(ids)\n",
    "                                print(f\"    Pred: {toks}\")\n",
    "            except Exception as e: print(f\"Eval Error: {e}\")\n",
    "            model.train()\n",
    "\n",
    "        # Checkpointing\n",
    "        if global_step % training_args.save_steps == 0:\n",
    "            ckpt_dir = os.path.join(training_args.output_dir, f\"checkpoint-{global_step}\")\n",
    "            print(f\"Saving to {ckpt_dir}...\")\n",
    "            accelerator.wait_for_everyone()\n",
    "            accelerator.save_state(ckpt_dir)\n",
    "            \n",
    "            if accelerator.is_main_process:\n",
    "                accelerator.save(optimizer.state_dict(), os.path.join(ckpt_dir, \"optimizer.pt\"))\n",
    "                accelerator.save(scheduler.state_dict(), os.path.join(ckpt_dir, \"scheduler.pt\"))\n",
    "                if scaler: accelerator.save(scaler.state_dict(), os.path.join(ckpt_dir, \"scaler.pt\"))\n",
    "                # Saving dataset states!\n",
    "                accelerator.save(combined_dataset.state_dict(), os.path.join(ckpt_dir, \"dataset_state.pt\"))\n",
    "                accelerator.save(train_dataloader.state_dict(), os.path.join(ckpt_dir, \"dataloader_state.pt\"))\n",
    "                tokenizer.save_pretrained(ckpt_dir)\n",
    "\n",
    "            # Cleanup old checkpoints to save space\n",
    "            if accelerator.is_main_process:\n",
    "                all_ckpts = sorted([d for d in os.listdir(training_args.output_dir) if d.startswith(\"checkpoint-\")])\n",
    "                if len(all_ckpts) > training_args.save_total_limit:\n",
    "                    for old in all_ckpts[:-training_args.save_total_limit]:\n",
    "                        shutil.rmtree(os.path.join(training_args.output_dir, old))\n",
    "                        print(f\"Deleted old: {old}\")\n",
    "\n",
    "print(\"Finished.\")\n",
    "accelerator.wait_for_everyone()\n",
    "accelerator.unwrap_model(model).save_pretrained(training_args.output_dir)\n",
    "if accelerator.is_main_process: tokenizer.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Final Inference Test\n",
    "The training is done. Now I load the saved model and give it a few simple tests. If it predicts reasonable words for the `<mask>` tokens, the pre-training worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Testing Final Model...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "try:\n",
    "    tokenizer_loaded = AutoTokenizer.from_pretrained(model_args.model_output_dir)\n",
    "    model_loaded = BartForConditionalGeneration.from_pretrained(model_args.model_output_dir)\n",
    "    model_loaded.eval().to(device)\n",
    "\n",
    "    test_sentences = [\n",
    "        f\"Hello world, this is a {tokenizer_loaded.mask_token} test.\",\n",
    "        f\"النهاردة الجو حر {tokenizer_loaded.mask_token}.\",\n",
    "        f\"I need to {tokenizer_loaded.mask_token} the results tomorrow.\",\n",
    "    ]\n",
    "\n",
    "    for sent in test_sentences:\n",
    "        print(f\"\\nInput: {sent}\")\n",
    "        enc = tokenizer_loaded(sent, return_tensors=\"pt\", truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model_loaded(**enc).logits\n",
    "        \n",
    "        mask_id = tokenizer_loaded.mask_token_id\n",
    "        mask_pos = (enc.input_ids == mask_id).nonzero(as_tuple=False)\n",
    "        \n",
    "        for m in mask_pos:\n",
    "            b, p = m[0], m[1]\n",
    "            vals, ids = torch.topk(logits[b, p], k=5)\n",
    "            toks = tokenizer_loaded.convert_ids_to_tokens(ids)\n",
    "            print(f\"  Prediction: {toks}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Test failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
