{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Shams\n",
    "\n",
    "**Description:**\n",
    "This is where I build the tokenizer from the ground up. I am not loading a pre-trained one. I am taking the raw BPE algorithm, adding the BART-specific rules (like using 'Metaspace' and the specific start/end tags), and training it on my cleaned data. This way it learns Egyptian Arabic correctly.\n",
    "\n",
    "**The making:**\n",
    "Instead of loading a ready-made tokenizer, I initialize a raw `BPE` model. I manually set the pre-tokenizer to `Metaspace` (which handles spaces as underscores, crucial for BART/RoBERTa). Then I define the Trainer with my vocabulary size (90k). Finally, and most importantly, I attach a `TemplateProcessing` post-processor that forces every sentence to look like `<s> sentence </s>`. This matches exactly what the BART model expects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports\n",
    "Installing the libraries and importing what I need. I use `tokenizers` for the heavy lifting and `transformers` to wrap the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tokenizers datasets transformers -q\n",
    "print(\"Libraries installed.\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from datasets import load_from_disk\n",
    "from tokenizers import (\n",
    "    Tokenizer,\n",
    "    models,\n",
    "    pre_tokenizers,\n",
    "    trainers,\n",
    "    processors,\n",
    "    decoders\n",
    ")\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuration\n",
    "Here I define the rules. I'm using a vocabulary of 90,000 words because Arabic is rich. I also define the special tokens exactly like the original BART model uses (`<s>`, `</s>`, etc.) so they are compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerConfig:\n",
    "    # simple flag to run a fast test if needed\n",
    "    IS_TEST_RUN = False\n",
    "    \n",
    "    # where the cleaned data lives\n",
    "    INPUT_DATA_PATH = \"/kaggle/input/spre-tokenization/MyPreTokenizedData\"\n",
    "    \n",
    "    # where to put the output\n",
    "    SAVE_DIR = \"/kaggle/working/ARZ-EN-BART-Tokenizer/\"\n",
    "    \n",
    "    TEXT_COLUMN = 'pretokenized'\n",
    "\n",
    "    # 90k is a good sweet spot for bilingual (ar/en) data\n",
    "    VOCAB_SIZE = 90000\n",
    "    MIN_FREQUENCY = 7\n",
    "    \n",
    "    # I split data into chunks to save RAM\n",
    "    ROWS_PER_CHUNK_FILE = 4_000_000\n",
    "    TEXT_CHUNK_DIR = \"/kaggle/working/training_text_chunks/\"\n",
    "\n",
    "    # BART uses these specific symbols. I need to match them.\n",
    "    UNK_TOKEN = \"<unk>\"\n    PAD_TOKEN = \"<pad>\"\n    MASK_TOKEN = \"<mask>\"\n    CLS_TOKEN = \"<s>\"    # Start of sentence\n    EOS_TOKEN = \"</s>\"   # End of sentence\n    \n    SPECIAL_TOKENS = [\n        UNK_TOKEN, PAD_TOKEN, MASK_TOKEN, CLS_TOKEN, EOS_TOKEN\n    ]\n\nprint(\"Config loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. File Streaming\n",
    "I can't load the whole dataset into RAM at once. This function reads the dataset row by row and writes it into smaller text files (`chunks`). The tokenizer will read these files one by one later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_files(config: TokenizerConfig) -> list[str]:\n",
    "    print(f\"\\nStreaming dataset to text files...\")\n",
    "    print(f\"  -> Loading from: {config.INPUT_DATA_PATH}\")\n",
    "    try:\n",
    "        main_dataset = load_from_disk(config.INPUT_DATA_PATH)\n",
    "        print(f\"  -> Loaded. Rows: {len(main_dataset)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Failed to load data. Error: {e}\")\n",
    "        return []\n",
    "\n",
    "    os.makedirs(config.TEXT_CHUNK_DIR, exist_ok=True)\n",
    "    file_paths = []\n",
    "    file_number = 1\n",
    "\n",
    "    print(f\"  -> Writing chunks to: {config.TEXT_CHUNK_DIR}\")\n",
    "    \n",
    "    f = open(os.path.join(config.TEXT_CHUNK_DIR, f\"chunk_{file_number}.txt\"), \"w\", encoding=\"utf-8\")\n",
    "    \n",
    "    for i, record in enumerate(main_dataset):\n",
    "        # stop early if this is just a test\n",
    "        if config.IS_TEST_RUN and i > 10_000_000:\n",
    "            print(f\"    -> Test mode: Stopping early.\")\n",
    "            break\n",
    "            \n        # close current file and start a new one if it gets too big\n",
    "        if i > 0 and i % config.ROWS_PER_CHUNK_FILE == 0:\n",
    "            f.close()\n",
    "            file_paths.append(f.name)\n",
    "            print(f\"    - Saved chunk: {os.path.basename(f.name)}\")\n",
    "            file_number += 1\n",
    "            f = open(os.path.join(config.TEXT_CHUNK_DIR, f\"chunk_{file_number}.txt\"), \"w\", encoding=\"utf-8\")\n",
    "        \n",
    "        # write the actual text to the file\n",
    "        text_list = record.get(config.TEXT_COLUMN)\n",
    "        if text_list and isinstance(text_list, list):\n",
    "            f.write(\" \".join(text_list) + \"\\n\")\n",
    "\n",
    "    f.close()\n",
    "    file_paths.append(f.name)\n",
    "    print(f\"    - Saved final chunk: {os.path.basename(f.name)}\")\n",
    "    print(f\"  -> Chunking done. Created {len(file_paths)} files.\")\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training Logic\n",
    "This is the core logic. \n",
    "1. I start a blank BPE tokenizer.\n",
    "2. I set `Metaspace` (this replaces spaces with a special underscore char, needed for RoBERTa/BART).\n",
    "3. I train it on the files we made above.\n",
    "4. **Crucial**: I attach a `post_processor`. This ensures every sentence automatically gets wrapped in `<s>` and `</s>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_tokenizer(config: TokenizerConfig, file_paths: list[str]):\n",
    "    print(f\"\\nTraining Tokenizer...\")\n",
    "    \n",
    "    # starting from scratch with BPE\n",
    "    backend_tokenizer = Tokenizer(models.BPE(unk_token=config.UNK_TOKEN))\n    \n    # bart/roberta need this specific pre-tokenizer to handle spaces correctly\n    backend_tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()\n    backend_tokenizer.decoder = decoders.Metaspace()\n\n    # setting up the training rules\n    trainer = trainers.BpeTrainer(\n        vocab_size=config.VOCAB_SIZE,\n        min_frequency=config.MIN_FREQUENCY,\n        special_tokens=config.SPECIAL_TOKENS\n    )\n\n    print(f\"  -> Training on {len(file_paths)} files...\")\n    start_time = time.perf_counter()\n    backend_tokenizer.train(files=file_paths, trainer=trainer)\n    end_time = time.perf_counter()\n    print(f\"  -> Training finished in {end_time - start_time:.2f}s\")\n\n    # finding the IDs for start and end tokens\n    cls_token_id = backend_tokenizer.token_to_id(config.CLS_TOKEN)\n    eos_token_id = backend_tokenizer.token_to_id(config.EOS_TOKEN)\n\n    if cls_token_id is None or eos_token_id is None:\n        raise RuntimeError(f\"Something went wrong. Special tokens missing.\")\n        \n    print(f\"  -> Adding BART post-processing rules...\")\n\n    # this template tells the tokenizer: \"put <s> at start and </s> at end\"\n    backend_tokenizer.post_processor = processors.TemplateProcessing(\n        single=f\"{config.CLS_TOKEN} $A {config.EOS_TOKEN}\",\n        pair=f\"{config.CLS_TOKEN} $A {config.EOS_TOKEN} {config.EOS_TOKEN} $B {config.EOS_TOKEN}\",\n        special_tokens=[\n            (config.CLS_TOKEN, cls_token_id),\n            (config.EOS_TOKEN, eos_token_id)\n        ],\n    )\n\n    print(f\"  -> Wrapping in Hugging Face format...\")\n    \n    final_tokenizer = PreTrainedTokenizerFast(\n        tokenizer_object=backend_tokenizer,\n        unk_token=config.UNK_TOKEN,\n        pad_token=config.PAD_TOKEN,\n        mask_token=config.MASK_TOKEN,\n        cls_token=config.CLS_TOKEN,\n        eos_token=config.EOS_TOKEN,\n        sep_token=config.EOS_TOKEN,      # bart uses eos as separator\n        bos_token=config.CLS_TOKEN,      # bart uses cls as beginning\n        add_prefix_space=True            # essential for bart\n    )\n\n    print(f\"  -> Vocab size: {len(final_tokenizer)}\")\n\n    os.makedirs(config.SAVE_DIR, exist_ok=True)\n    print(f\"  -> Saving to: {config.SAVE_DIR}\")\n    \n    # saving the raw backend JSON\n    backend_tokenizer.save(os.path.join(config.SAVE_DIR, \"tokenizer.json\"))\n    \n    # saving the friendly HF config files\n    final_tokenizer.save_pretrained(config.SAVE_DIR)\n    \n    print(f\"  -> Saved. Ready for verification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Main Execution\n",
    "Running the whole pipeline: config, chunking, training, and cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"===== BART Tokenizer Training =====\")\n",
    "    total_start_time = time.perf_counter()\n    \n    if config.IS_TEST_RUN:\n        print(\"  -> Mode: Test Run\")\n    else:\n        print(\"  -> Mode: Production Run\")\n    print(f\"  -> Output: {config.SAVE_DIR}\")\n\n    try:\n        # step 1: create text files\n        training_files = get_training_files(config)\n        \n        if not training_files:\n            raise RuntimeError(\"No files created.\")\n\n        # step 2: train\n        train_and_save_tokenizer(config, training_files)\n\n    except Exception as e:\n        print(f\"\\n--- CRASHED ---\")\n        print(f\"  -> {e}\")\n    \n    finally:\n        # step 3: cleanup\n        print(f\"\\n--- Cleanup ---\")\n        if os.path.exists(config.TEXT_CHUNK_DIR):\n            shutil.rmtree(config.TEXT_CHUNK_DIR)\n            print(f\"  -> Removed temp files.\")\n        else:\n            print(f\"  -> Nothing to clean.\")\n\n    total_end_time = time.perf_counter()\n    print(f\"\\n===== Finished in {total_end_time - total_start_time:.2f}s =====\")\n\n\n# create config and run\nconfig = TokenizerConfig()\nif __name__ == \"__main__\":\n    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Verification Tests\n",
    "I don't blindly trust code. I need to load the saved tokenizer and run tests.\n",
    "1. **Load Test**: Does it open?\n",
    "2. **Special Tokens**: Are `<s>` and `</s>` mapped correctly?\n",
    "3. **Round-Trip**: Can I encode a weird sentence and get the exact same string back?\n",
    "4. **Pair Test**: If I give it two sentences, does it put the separators `</s> </s>` in the middle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import json\n",
    "\n",
    "# --- CHECK CONFIG ---\n",
    "try:\n",
    "    TOKENIZER_PATH = config.SAVE_DIR\n",
    "    print(f\"  -> Using path from memory: {TOKENIZER_PATH}\")\n",
    "except NameError:\n",
    "    TOKENIZER_PATH = \"/kaggle/working/ARZ-EN-BART-Tokenizer/\"\n",
    "    print(f\"  -> Using default path: {TOKENIZER_PATH}\")\n",
    "\n",
    "if not os.path.exists(TOKENIZER_PATH):\n",
    "    print(f\"Directory not found. Run training first.\")\n",
    "else:\n",
    "    try:\n",
    "        # --- 1. LOAD TEST ---\n",
    "        print(\"\\n--- [1] Loading ---\")\n        tokenizer = PreTrainedTokenizerFast.from_pretrained(TOKENIZER_PATH)\n        print(f\"  -> Loaded. Vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "        # --- 2. SPECIAL TOKENS TEST ---\n",
    "        print(\"\\n--- [2] Checking Special Tokens ---\")\n        assert tokenizer.unk_token == \"<unk>\"\n        assert tokenizer.pad_token == \"<pad>\"\n        assert tokenizer.mask_token == \"<mask>\"\n        assert tokenizer.cls_token == \"<s>\"\n        assert tokenizer.eos_token == \"</s>\"\n        # check aliases\n        assert tokenizer.bos_token == tokenizer.cls_token\n        assert tokenizer.sep_token == tokenizer.eos_token\n        print(\"  -> Tokens match BART standard.\")\n        print(f\"  -> IDs: CLS={tokenizer.cls_token_id}, EOS={tokenizer.eos_token_id}\")\n\n        # --- 3. ROUND TRIP TEST ---\n        print(\"\\n--- [3] Round-Trip Tests ---\")\n        \n        complex_sentences = [\n            \"المفروض يعني محدش يتناك على التاني لحد مانلاقي حل في الموضوع ابن العرص دا 12345.\",\n            \"The tokenizer's interoperability, tested circa 2024-2025, must be 100% perfect.\",\n            \"!@#$%^&*_)(+1548/*@يىتىيسالةيكنةبىسنمf dkjn vjkfnvjdsvsnfvjfbvjbfvhdsvkdsvnfvo   ufnf\",\n            \"I told him to 'stop' 5 times, بس هو مصمم يعك الدنيا.\",\n            \"اللامركزية في البلوكتشين بتدينا شفافية ومحدش يقدر يتحكم في الداتا لوحده.\"\n        ]\n        \n        for i, text in enumerate(complex_sentences):\n            # encode then decode\n            encoded_ids = tokenizer.encode(text)\n            decoded = tokenizer.decode(encoded_ids, skip_special_tokens=True)\n            \n            # verify they are identical\n            if decoded != text:\n                 print(f\"Mismatch in test {i+1}!\")\n                 print(f\"Orig: {text}\")\n                 print(f\"Decoded: {decoded}\")\n                 raise AssertionError(\"Round-trip failed.\")\n            print(f\"  -> Test {i+1} passed.\")\n\n        print(\"  -> All text tests passed.\")\n\n\n        # --- 4. TEMPLATE TEST ---\n        print(\"\\n--- [4] Template Test ---\")\n        text_pair_1 = \"First part.\"\n        text_pair_2 = \"الجزء التاني.\"\n        encoded_pair = tokenizer(text_pair_1, text_pair_2)\n        \n        ids = encoded_pair['input_ids']\n        eos_id = tokenizer.eos_token_id\n        cls_id = tokenizer.cls_token_id\n        \n        # look for double EOS in the middle\n        double_eos_index = -1\n        for i in range(len(ids) - 1):\n            if ids[i] == eos_id and ids[i+1] == eos_id:\n                double_eos_index = i\n                break\n                \n        assert ids[0] == cls_id, \"Start token missing\"\n        assert ids[-1] == eos_id, \"End token missing\"\n        assert double_eos_index != -1, \"Separator (</s> </s>) missing\"\n        print(f\"  -> Found separator at index {double_eos_index}.\")\n\n        print(\"\\n===== VERIFIED =====\")\n\n    except Exception as e:\n        print(f\"\\n--- TEST FAILED ---\")\n        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Internal Inspection\n",
    "Just to be absolutely sure, I'm opening the JSON files themselves. I need to verify that `add_prefix_space` is `true` inside `tokenizer.json`, otherwise, the model will fail silently during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "print(\"===== Internal JSON Check =====\")\n",
    "\n",
    "# get path\n",
    "try:\n",
    "    TOKENIZER_PATH = config.SAVE_DIR\n",
    "except NameError:\n",
    "    TOKENIZER_PATH = \"/kaggle/working/ARZ-EN-BART-Tokenizer/\"\n",
    "\n",
    "# 1. Check tokenizer_config.json\n",
    "config_path = os.path.join(TOKENIZER_PATH, \"tokenizer_config.json\")\n",
    "try:\n",
    "    print(\"--- tokenizer_config.json ---\")\n",
    "    with open(config_path, 'r') as f:\n        config_data = json.load(f)\n    \n    if config_data.get(\"add_prefix_space\") is True:\n        print(\"  -> add_prefix_space is True (Good).\")\n    else:\n        print(\"  -> WARNING: add_prefix_space is NOT True.\")\nexcept Exception as e:\n    print(f\"Error reading config: {e}\")\n\n\n# 2. Check tokenizer.json internals\n",
    "core_path = os.path.join(TOKENIZER_PATH, \"tokenizer.json\")\n",
    "try:\n",
    "    print(\"\\n--- tokenizer.json ---\")\n",
    "    with open(core_path, 'r') as f:\n        core_data = json.load(f)\n\n    # check pre-tokenizer type\n    pre_tok_type = core_data.get('pre_tokenizer', {}).get('type')\n    if pre_tok_type == \"Metaspace\":\n         print(\"  -> Pre-tokenizer is Metaspace (Good).\")\n    else:\n         print(f\"  -> WARNING: Pre-tokenizer is {pre_tok_type}.\")\n\n    # check post-processor logic for the pairs\n    # we expect the pair template to include special tokens for A and B\n    post_processor = core_data.get('post_processor', {})\n    try:\n        pair_template = post_processor.get('pair', [])\n        # just checking if we have enough items in the template list to constitute a full structure\n        if len(pair_template) >= 5:\n             print(\"  -> Post-processor template looks correct.\")\n        else:\n             print(\"  -> WARNING: Post-processor template looks too short.\")\n    except Exception:\n        print(\"  -> Error checking post-processor.\")\n\nexcept Exception as e:\n    print(f\"Error reading core file: {e}\")\n\nprint(\"\\nDone checking internals.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
