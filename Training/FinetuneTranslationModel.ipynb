{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Shams\n",
    "\n",
    "**Description:**\n",
    "This notebook handles the **fine-tuning** phase. We have a pre-trained Encoder-Decoder model (from the previous step) that understands English and Arabic generally. Now, we are teaching it specifically how to translate English into Egyptian Arabic.\n",
    "\n",
    "**Key Strategy:**\n",
    "\n",
    "1.  **Loading the Pre-trained Checkpoint:**\n",
    "    We start with the weights from the pre-training phase. This model has seen a lot of text but hasn't been explicitly told to \"translate X to Y\".\n",
    "\n",
    "2.  **Architecture Patch (RMSNorm):**\n",
    "    Just like in pre-training, we must manually replace `LayerNorm` with `RMSNorm` *immediately* after loading the config. If we don't do this, the weights we load won't match the model structure, and everything will break.\n",
    "\n",
    "3.  **Data Pipeline:**\n",
    "    We load the parallel splits (`parallel_EN`, `parallel_ARZ`, etc.) from the dataset. We pair them up row-by-row to create a translation dataset. The `DataCollator` pads batches dynamically.\n",
    "\n",
    "4.  **Metric (SacreBLEU):**\n",
    "    Accuracy doesn't work for translation. We use BLEU score (specifically `sacrebleu`) to measure how close the model's output is to the human reference. We also track `gen_len` to make sure the model isn't just outputting empty strings.\n",
    "\n",
    "5.  **Generation Config:**\n",
    "    We explicitly set `num_beams=5` and `repetition_penalty=1.5`. This stops the model from getting stuck in loops (e.g., \"The car the car the car\") and forces it to explore better translations.\n",
    "\n",
    "6.  **The \"Lobotomy\" Fix:**\n",
    "    There is a known issue where loading weights can sometimes untie the input/output embeddings. We have a manual fix in the loading cell to ensure `embed_tokens` and `lm_head` share the same memory, keeping the model smart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Installs & Imports\n",
    "Setting up the fine-tuning environment. We need `sacrebleu` for metrics and `safetensors` for efficient weight loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.6)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.57.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.4.1)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.12.0)\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.20.0)\nRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.48.2)\nRequirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.7.0)\n..."
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install --upgrade evaluate transformers datasets accelerate tensorboard bitsandbytes sacrebleu safetensors\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import dataclass, field \n",
    "from datasets import load_dataset, concatenate_datasets, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState\n",
    ")\n",
    "from typing import Dict, List, Any\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# Suppress warnings for cleaner logs\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuration\n",
    "Defining where to find the pre-trained model and how to train it.\n",
    "- **ModelConfig:** Points to the `checkpoint-2000` from the pre-training run.\n",
    "- **TrainingConfig:** Sets the batch size (16) and epochs (10). `IN_TEST_MODE` allows for a quick 8000-sample run to verify the pipeline before committing to a full train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--- Configuration ---\nLoading pre-trained model from: /kaggle/input/lasthopr/bart-en-arz-translator/checkpoint-2000\nTraining for 1 epochs.\nBatch size: 17 (Accum: 1)\nMax Length: 150\n"
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    PRE_TRAINED_MODEL_PATH: str = \"/kaggle/input/lasthopr/bart-en-arz-translator/checkpoint-2000\"\n",
    "    DATASET_REPO_ID: str = \"Shams03/Tokenized-ARZ-EN-BART\"\n",
    "    FINETUNED_OUTPUT_DIR: str = \"/kaggle/working/bart-en-arz-translator\"\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    MAX_LENGTH: int = 150\n",
    "    LEARNING_RATE: float = 1e-5\n",
    "    NUM_EPOCHS: int = 10\n",
    "    PER_DEVICE_BATCH_SIZE: int = 16\n",
    "    GRAD_ACCUMULATION_STEPS: int = 2\n",
    "    EVAL_STEPS: int = 500\n",
    "    SAVE_STEPS: int = 2000\n",
    "    TEST_SPLIT_SIZE: float = 0.01\n",
    "    LOGGING_STEPS = 100\n",
    "\n",
    "    IN_TEST_MODE: bool = False \n",
    "    TEST_MODE_DATA_SIZE: int = 8000 \n",
    "\n",
    "model_config = ModelConfig()\n",
    "training_config = TrainingConfig()\n",
    "\n",
    "print(f\"--- Configuration ---\")\n",
    "print(f\"Loading pre-trained model from: {model_config.PRE_TRAINED_MODEL_PATH}\")\n",
    "print(f\"Training for {training_config.NUM_EPOCHS} epochs.\")\n",
    "print(f\"Batch size: {training_config.PER_DEVICE_BATCH_SIZE} (Accum: {training_config.GRAD_ACCUMULATION_STEPS})\")\n",
    "print(f\"Max Length: {training_config.MAX_LENGTH}\")\n",
    "\n",
    "if training_config.IN_TEST_MODE:\n",
    "    print(\"      !!! TEST MODE ON !!!     \")\n",
    "    print(f\"Data will be limited to {training_config.TEST_MODE_DATA_SIZE} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Model & Apply Architecture Patch\n",
    "This cell does three critical things:\n",
    "1.  **Loads Config:** Reads the JSON config from the checkpoint.\n",
    "2.  **Patches Architecture:** Immediately replaces `LayerNorm` with `RMSNorm`. The weights on disk correspond to an RMSNorm model; if we load them into a standard LayerNorm model, the shapes will match but the math will be wrong.\n",
    "3.  **Repairs Embeddings:** Manually forces the encoder, decoder, and language head embeddings to share memory (`shared_param`). This fixes the \"lobotomy\" issue where the model forgets language associations after loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n--- 1. Loading Model & Tokenizer ---\nLoading tokenizer from: /kaggle/input/lasthopr/bart-en-arz-translator/checkpoint-2000\nLoading model CONFIG from: /kaggle/input/lasthopr/bart-en-arz-translator/checkpoint-2000\nPatching model architecture: Replacing LayerNorm with RMSNorm...\nLoading saved weights from .safetensors file...\nFound shared weight key: 'model.shared.weight' - Applying Fix...\nSuccess: Embeddings manually tied and conflicting keys removed.\nLoading remaining weights...\nModel loaded successfully.\n\nModel: <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>\nTokenizer: <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n"
    }
   ],
   "source": [
    "print(f\"\\n--- 1. Loading Model & Tokenizer ---\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "from transformers import AutoTokenizer, BartConfig, AutoModelForSeq2SeqLM\n",
    "\n",
    "# --- A. Define RMSNorm ---\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return x / rms * self.scale\n",
    "\n",
    "# --- B. Replacement Function ---\n",
    "def replace_layernorm_with_rmsnorm(module: nn.Module):\n",
    "    for name, child in list(module.named_children()):\n",
    "        if isinstance(child, nn.LayerNorm):\n",
    "            dim = child.normalized_shape[0] if isinstance(child.normalized_shape, (tuple, list)) else child.normalized_shape\n",
    "            rms = RMSNorm(dim=dim, eps=1e-6)\n",
    "            setattr(module, name, rms)\n",
    "        else:\n",
    "            replace_layernorm_with_rmsnorm(child)\n",
    "\n",
    "# --- C. Load Tokenizer ---\n",
    "print(f\"Loading tokenizer from: {model_config.PRE_TRAINED_MODEL_PATH}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_config.PRE_TRAINED_MODEL_PATH,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "\n",
    "# --- D. Load Model Config & Patch ---\n",
    "print(f\"Loading model CONFIG from: {model_config.PRE_TRAINED_MODEL_PATH}\")\n",
    "config = BartConfig.from_pretrained(model_config.PRE_TRAINED_MODEL_PATH)\n",
    "model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "\n",
    "print(\"Patching model architecture: Replacing LayerNorm with RMSNorm...\")\n",
    "replace_layernorm_with_rmsnorm(model)\n",
    "\n",
    "# --- E. Load Weights & Fix Embeddings ---\n",
    "print(\"Loading saved weights from .safetensors file...\")\n",
    "state_dict_path = os.path.join(model_config.PRE_TRAINED_MODEL_PATH, \"model.safetensors\")\n",
    "state_dict = load_file(state_dict_path, device=\"cpu\")\n",
    "\n",
    "shared_key = 'model.shared.weight'\n",
    "if shared_key not in state_dict:\n",
    "    shared_key = 'shared.weight'\n",
    "\n",
    "if shared_key in state_dict:\n",
    "    print(f\"Found shared weight key: '{shared_key}' - Applying Fix...\")\n",
    "    shared_weight = state_dict[shared_key]\n",
    "    \n",
    "    # Tie embeddings manually\n",
    "    shared_param = nn.Parameter(shared_weight)\n",
    "    model.model.encoder.embed_tokens.weight = shared_param\n",
    "    model.model.decoder.embed_tokens.weight = shared_param\n",
    "    model.lm_head.weight = shared_param\n",
    "    \n",
    "    # Remove redundant keys from dict to prevent overwrite\n",
    "    keys_to_remove = [\n",
    "        shared_key,\n",
    "        \"model.encoder.embed_tokens.weight\",\n",
    "        \"model.decoder.embed_tokens.weight\",\n",
    "        \"lm_head.weight\"\n",
    "    ]\n",
    "    for k in keys_to_remove:\n",
    "        if k in state_dict:\n",
    "            del state_dict[k]\n",
    "            \n",
    "    print(\"Success: Embeddings manually tied and conflicting keys removed.\")\n",
    "else:\n",
    "    print(\"WARNING: Shared weights not found in file!\")\n",
    "\n",
    "print(\"Loading remaining weights...\")\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "print(f\"\\nModel: {type(model)}\")\n",
    "print(f\"Tokenizer: {type(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Preparation\n",
    "Here, we fetch the parallel datasets. The English and Arabic data are stored in separate splits. We load them, detokenize them (turn them back into raw text), and then join them side-by-side into a single dataset with `en` and `arz` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n--- 2. Reconstructing Parallel Data (Optimized) ---\nLoading from repo: Shams03/Tokenized-ARZ-EN-BART\nLoading splits: ['parallel_EN', 'parallel_ARZ', 'LparallelEN', 'Lparallel_ARZ']...\n...Splits loaded.\nTotal parallel sentences loaded: 704350\nDetokenizing English data (using 4 cores)...\nDetokenizing Arabic data (using 4 cores)...\nCreating final paired dataset (using concatenate_datasets axis=1)...\n...Dataset paired successfully.\nCreating train/validation split (Test size: 0.01)...\n\n--- Dataset Reconstruction Complete ---\nDatasetDict({\n    train: Dataset({\n        features: ['en', 'arz'],\n        num_rows: 697306\n    })\n    test: Dataset({\n        features: ['en', 'arz'],\n        num_rows: 7044\n    })\n})\n\nExample:\n  EN: 8 month later I walked out the hospital on my own two feet\n  ARZ: بعد 8 شهور خرجت من ال مستشفى على رجليا\n"
    }
   ],
   "source": [
    "print(f\"\\n--- 2. Reconstructing Parallel Data (Optimized) ---\")\n",
    "print(f\"Loading from repo: {model_config.DATASET_REPO_ID}\")\n",
    "\n",
    "splits_to_load = ['parallel_EN', 'parallel_ARZ', 'LparallelEN', 'Lparallel_ARZ']\n",
    "print(f\"Loading splits: {splits_to_load}...\")\n",
    "\n",
    "ds = load_dataset(model_config.DATASET_REPO_ID, split=splits_to_load, streaming=False)\n",
    "print(\"...Splits loaded.\")\n",
    "\n",
    "# Concatenate splits\n",
    "ds_en_tokenized = concatenate_datasets([ds[0], ds[2]]) \n",
    "ds_arz_tokenized = concatenate_datasets([ds[1], ds[3]]) \n",
    "\n",
    "if training_config.IN_TEST_MODE:\n",
    "    print(f\"\\n--- TEST MODE: Slicing raw data to {training_config.TEST_MODE_DATA_SIZE} samples ---\")\n",
    "    ds_en_tokenized = ds_en_tokenized.select(range(training_config.TEST_MODE_DATA_SIZE))\n",
    "    ds_arz_tokenized = ds_arz_tokenized.select(range(training_config.TEST_MODE_DATA_SIZE))\n",
    "\n",
    "if len(ds_en_tokenized) != len(ds_arz_tokenized):\n",
    "    raise ValueError(\"Data mismatch! Unequal EN/ARZ rows.\")\n",
    "else:\n",
    "    print(f\"Total parallel sentences loaded: {len(ds_en_tokenized)}\")\n",
    "\n",
    "def detokenize(example):\n",
    "    return {\"text\": tokenizer.decode(example['input_ids'], skip_special_tokens=True)}\n",
    "\n",
    "# Detokenize in parallel\n",
    "print(f\"Detokenizing English data (using {os.cpu_count()} cores)...\")\n",
    "ds_en_text = ds_en_tokenized.map(detokenize, num_proc=os.cpu_count(), remove_columns=ds_en_tokenized.column_names)\n",
    "\n",
    "print(f\"Detokenizing Arabic data (using {os.cpu_count()} cores)...\")\n",
    "ds_arz_text = ds_arz_tokenized.map(detokenize, num_proc=os.cpu_count(), remove_columns=ds_arz_tokenized.column_names)\n",
    "\n",
    "# Join datasets\n",
    "print(\"Creating final paired dataset...\")\n",
    "ds_en_text = ds_en_text.rename_column(\"text\", \"en\")\n",
    "ds_arz_text = ds_arz_text.rename_column(\"text\", \"arz\")\n",
    "paired_data = concatenate_datasets([ds_en_text, ds_arz_text], axis=1)\n",
    "\n",
    "print(\"...Dataset paired successfully.\")\n",
    "\n",
    "# Split train/test\n",
    "print(f\"Creating train/validation split (Test size: {training_config.TEST_SPLIT_SIZE})...\")\n",
    "raw_datasets = paired_data.train_test_split(test_size=training_config.TEST_SPLIT_SIZE, seed=42)\n",
    "\n",
    "# Cleanup RAM\n",
    "del ds, ds_en_tokenized, ds_arz_tokenized, ds_en_text, ds_arz_text, paired_data\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n--- Dataset Reconstruction Complete ---\")\n",
    "print(raw_datasets)\n",
    "print(\"\\nExample:\")\n",
    "print(f\"  EN: {raw_datasets['train'][0]['en']}\")\n",
    "print(f\"  ARZ: {raw_datasets['train'][0]['arz']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Seq2Seq Tokenization\n",
    "Now we tokenize properly for the Seq2Seq task. \n",
    "- The **Encoder** gets the English text (`input_ids`).\n",
    "- The **Decoder** gets the Arabic text (`labels`).\n",
    "\n",
    "We use `tokenizer.as_target_tokenizer()` for the labels to ensure correct special token handling if the tokenizer distinguishes between source/target (though BPE usually shares them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n--- 3. Applying Seq2Seq Tokenization ---\n\n--- Tokenization Complete ---\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 697306\n    })\n    test: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 7044\n    })\n})\n\nExample of tokenized data:\n  input_ids: [3, 1837, 2442, 2777, 637, 4800, 863, 603, 6673, 674, 895, 1350, 1469, 4409, 4]...\n  labels: [3, 1295, 1837, 9660, 21647, 669, 599, 10767, 812, 2191, 9004, 4]...\n"
    }
   ],
   "source": [
    "print(f\"\\n--- 3. Applying Seq2Seq Tokenization ---\")\n",
    "MAX_LENGTH = training_config.MAX_LENGTH\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize Source (English)\n",
    "    inputs = tokenizer(\n",
    "        examples[\"en\"], \n",
    "        max_length=MAX_LENGTH, \n",
    "        truncation=True, \n",
    "        padding=False \n",
    "    )\n",
    "\n",
    "    # Tokenize Target (Arabic)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"arz\"], \n",
    "            max_length=MAX_LENGTH, \n",
    "            truncation=True, \n",
    "            padding=False\n",
    "        )\n",
    "\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n    batched=True,\n",
    "    num_proc=os.cpu_count(),\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(\"\\n--- Tokenization Complete ---\")\n",
    "print(tokenized_datasets)\n",
    "print(\"\\nExample of tokenized data:\")\n",
    "print(f\"  input_ids: {tokenized_datasets['train'][0]['input_ids'][:20]}...\")\n",
    "print(f\"  labels: {tokenized_datasets['train'][0]['labels'][:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Collator & Metrics\n",
    "We initialize the `DataCollatorForSeq2Seq` which handles dynamic padding (so we don't waste GPU memory on empty space).\n",
    "\n",
    "The `compute_metrics` function calculates the **BLEU score**. It also has a safety clip (`np.clip`) to prevent crashes if the model predicts a token ID larger than the vocabulary size (a rare but fatal bug)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n--- 5. Initializing Collator & Metric ---\nDataCollatorForSeq2Seq initialized.\nSacreBLEU metric ready.\n"
    }
   ],
   "source": [
    "print(f\"\\n--- 5. Initializing Collator & Metric ---\")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    model=model,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "print(\"DataCollatorForSeq2Seq initialized.\")\n",
    "\n",
    "import evaluate\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "\n",
    "    if hasattr(preds, \"ndim\") and preds.ndim == 3:\n",
    "        preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    # Safety clip to vocab size\n",
    "    max_valid_id = tokenizer.vocab_size - 1\n",
    "    np.clip(preds, 0, max_valid_id, out=preds)\n",
    "\n",
    "    # Replace -100 with pad token for decoding\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_labels_for_bleu = [[lbl] for lbl in decoded_labels]\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels_for_bleu)\n",
    "\n",
    "    if \"score\" in result:\n",
    "        result[\"bleu\"] = result.pop(\"score\")\n",
    "\n",
    "    final_metrics = {k: round(float(v), 4) for k, v in result.items() if isinstance(v, (int, float))}\n",
    "    return final_metrics\n",
    "\n",
    "print(\"SacreBLEU metric ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Debug Callback\n",
    "This is a custom callback I wrote to see what the model is actually doing during training. Every `logging_steps` (100), it pauses and translates a list of specific test sentences (\"Get in the car\", \"I see dead people\", etc.). This lets me visually verify if the model is learning or just outputting nonsense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n--- 6. Defining Debugging Callback ---\n\nTranslationLogCallback defined.\n"
    }
   ],
   "source": [
    "print(f\"\\n--- 6. Defining Debugging Callback ---\\n\")\n",
    "\n",
    "DEBUG_EXAMPLES = {\n",
    "    \"test_1_car\": \"Get in the car, we have to go now!\",\n",
    "    \"test_2_feeling\": \"I have a very bad feeling about this.\",\n",
    "    \"cmd_1_door\": \"Open the door, please.\",\n",
    "    \"movie_1_godfather\": \"I'm gonna make him an offer he can't refuse.\",\n",
    "    \"chat_1_greeting\": \"Hello, how are you today?\",\n",
    "    \"hard_2_idiom\": \"It's raining cats and dogs.\"\n",
    "}\n",
    "\n",
    "class TranslationLogCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, model, max_length=128):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.max_length = max_length\n",
    "        self.debug_examples = DEBUG_EXAMPLES\n",
    "\n",
    "    def on_log(self, args: Seq2SeqTrainingArguments, state: TrainerState, control: TrainerControl, logs: Dict[str, float] = None, **kwargs):\n",
    "        if not state.is_world_process_zero or logs is None:\n",
    "            return\n",
    "\n",
    "        # Log training samples\n",
    "        if \"loss\" in logs and \"eval_loss\" not in logs:\n",
    "            print(f\"\\n--- [Debug Translation @ Step {state.global_step}] ---\")\n",
    "            current_lr = logs.get(\"learning_rate\", \"N/A\")\n",
    "            train_loss = logs.get(\"loss\", \"N/A\")\n",
    "            print(f\"  [Log] Step: {state.global_step} | Loss: {train_loss} | LR: {current_lr}\")\n",
    "            \n",
    "            self.model.eval()\n",
    "            for name, text in self.debug_examples.items():\n",
    "                try:\n",
    "                    inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=self.max_length).to(self.model.device)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.model.generate(**inputs, max_new_tokens=self.max_length, num_beams=4)\n",
    "                    translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                    print(f\"  [EN] {text}\\n  [ARZ] {translation}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error: {e}\")\n",
    "            self.model.train()\n",
    "            print(\"[End Debug]\")\n",
    "\n",
    "        # Log eval metrics\n",
    "        if \"eval_loss\" in logs:\n",
    "            print(f\"\\n--- [Evaluation @ Step {state.global_step}] ---\")\n",
    "            for k, v in logs.items():\n",
    "                if k != 'epoch':\n",
    "                    print(f\"  {k}: {v}\")\n",
    "            print(\"--------------------------\")\n",
    "\n",
    "print(\"TranslationLogCallback defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Training Arguments\n",
    "Configuring the `Seq2SeqTrainer`.\n",
    "- **Generation Config:** `num_beams=5` is key for translation quality. `repetition_penalty=1.5` prevents stuttering.\n",
    "- **FP16:** Enabled for speed.\n",
    "- **Early Stopping:** We stop training if the BLEU score doesn't improve for 5 evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n--- 7. Defining Training Arguments ---\n\nTraining Arguments set. Output will be saved to: /kaggle/working/bart-en-arz-translator\nEvaluation and Logging will happen every 100 and 100 steps.\nGenerationConfig set: max_new_tokens=128, num_beams=5\nload_best_model_at_end=True. Early stopping will be added in the next cell (Patience=5).\n"
    }
   ],
   "source": [
    "print(f\"\\n--- 7. Defining Training Arguments ---\\n\")\n",
    "\n",
    "from transformers import GenerationConfig, EarlyStoppingCallback\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 5 \n",
    "GENERATION_MAX_NEW_TOKENS = 128 \n",
    "GENERATION_NUM_BEAMS = 5 \n",
    "\n",
    "gen_config = GenerationConfig.from_model_config(model.config)\n",
    "gen_config.max_new_tokens = GENERATION_MAX_NEW_TOKENS\n",
    "gen_config.num_beams = GENERATION_NUM_BEAMS\n",
    "gen_config.no_repeat_ngram_size = 3\n",
    "gen_config.repetition_penalty = 1.5\n",
    "gen_config.length_penalty = 1.2\n",
    "gen_config.early_stopping = True\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_config.FINETUNED_OUTPUT_DIR,\n",
    "    learning_rate=training_config.LEARNING_RATE,\n",
    "    num_train_epochs=training_config.NUM_EPOCHS,\n",
    "    per_device_train_batch_size=training_config.PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=training_config.GRAD_ACCUMULATION_STEPS,\n",
    "    logging_steps=training_config.LOGGING_STEPS,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=training_config.EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=training_config.SAVE_STEPS,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    fp16=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_config=gen_config,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "translation_logger = TranslationLogCallback(tokenizer=tokenizer, model=model, max_length=training_config.MAX_LENGTH)\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[translation_logger, early_stopping_callback]\n",
    ")\n",
    "\n",
    "print(f\"Training Arguments set. Output will be saved to: {training_args.output_dir}\")\n",
    "print(f\"Evaluation and Logging will happen every {training_args.eval_steps} and {training_args.logging_steps} steps.\")\n",
    "print(f\"GenerationConfig set: max_new_tokens={GENERATION_MAX_NEW_TOKENS}, num_beams={GENERATION_NUM_BEAMS}\")\n",
    "print(f\"load_best_model_at_end=True. Early stopping will be added in the next cell (Patience={EARLY_STOPPING_PATIENCE}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Training Loop\n",
    "This is where the magic happens. I'm running a fresh training session. I'm manually loading the weights via `load_state_dict` instead of relying on `resume_from_checkpoint` because the optimizer state in the checkpoint might be incompatible with the new fine-tuning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Recovering weights from: /kaggle/input/lasthopr/bart-en-arz-translator/checkpoint-2000\nLoading .safetensors...\nWeights loaded. Missing keys: ['model.decoder.embed_tokens.weight', 'lm_head.weight']\nStarting training with FRESH optimizer (ignoring broken optimizer.pt)...\n"
    },
    {
     "data": {
      "text/html": "\n    <div>\n      \n      <progress value='101' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 00:42, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='29' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 29/126 04:23 < 15:13, 0.11 it/s]\n    </div>\n    ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n--- [Debug Translation @ Step 100] ---\n  [Log] Step: 100 | Epoch: 0.00 | LR: 1.00e-07 | Training Loss: 2.3525\n  [EN] Input (test_1_car): 'Get in the car, we have to go now!'\n  [ARZ] Output: 'ادخل في ال عربية ال لي لازم نمشي دلوقتي'\n  [EN] Input (test_2_feeling): 'I have a very bad feeling about this.'\n  [ARZ] Output: 'عندي إحساس وحش أوي في ال موضوع ده'\n  [EN] Input (test_3_science): 'The process of photosynthesis converts light energy into chemical energy.'\n  [ARZ] Output: 'عملية طاقة ال صور ال لي بتدمج ال ضوء ال بصري في ال طاقة ال كيميائية'\n  [EN] Input (test_4_question): 'What are you doing here?'\n  [ARZ] Output: 'بتعمل إيه هنا ؟'\n  [EN] Input (cmd_1_door): 'Open the door, please.'\n  [ARZ] Output: 'افتح ال باب من فضلك'\n  [EN] Input (cmd_2_phone): 'Give me the phone.'\n  [ARZ] Output: 'اديني ال تليفون'\n  [EN] Input (cmd_3_police): 'Call the police!'\n  [ARZ] Output: 'كلم ال بوليس'\n  [EN] Input (cmd_4_lights): 'Turn off the lights before you leave.'\n  [ARZ] Output: 'اقفل ال أنوار قبل ما تمشي'\n  [EN] Input (movie_1_godfather): 'I'm gonna make him an offer he can't refuse.'\n  [ARZ] Output: 'أنا هخليه عرض مقدرش يرفض نفسه'\n  [EN] Input (movie_2_gone_wind): 'Frankly, my dear, I don't give a damn.'\n  [ARZ] Output: 'بصراحة أنا مش فارق معايا يا حبيبتي . أنا مش مهتم أوي'\n  [EN] Input (movie_3_taxi): 'You talkin' to me?'\n  [ARZ] Output: 'انت بتتكلم معايا يا صاحبي'\n  [EN] Input (movie_4_starwars): 'I am your father.'\n  [ARZ] Output: 'أنا أبوكي'\n  [EN] Input (movie_5_sixth_sense): 'I see dead people.'\n  [ARZ] Output: 'أنا شايف ناس ميتة'\n  [EN] Input (chat_1_greeting): 'Hello, how are you today?'\n  [ARZ] Output: 'أهلًا ألو ، إزيك ال نهاردة ؟'\n  [EN] Input (chat_2_intro): 'My name is Ahmed and I am from Cairo.'\n  [ARZ] Output: 'أنا اسمي أحمدي نجاد وأنا من ال قاهرة'\n  [EN] Input (chat_3_location): 'Where is the nearest hospital?'\n  [ARZ] Output: 'فين أقرب مستشفى مستشفى ؟'\n  [EN] Input (chat_4_emotion): 'I am very happy to see you.'\n  [ARZ] Output: 'أنا مبسوط أوي إني شفتك'\n  [EN] Input (hard_1_travel): 'I need to book a flight to Alexandria.'\n  [ARZ] Output: 'محتاج أقرأ طيارة لل إسكندرية'\n  [EN] Input (hard_2_idiom): 'It's raining cats and dogs.'\n  [ARZ] Output: 'بتمطر ال قطط وال كلاب'\n  [EN] Input (hard_3_slang): 'This is fucking awesome, man!'\n  [ARZ] Output: 'ده راجل رائع فشخ'\n[End Debug Translation]\n"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import Seq2SeqTrainer\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "CKPT_PATH = \"/kaggle/input/lasthopr/bart-en-arz-translator/checkpoint-2000\"\n",
    "print(f\"Recovering weights from: {CKPT_PATH}\")\n",
    "\n",
    "weights_path = os.path.join(CKPT_PATH, \"model.safetensors\")\n",
    "if os.path.exists(weights_path):\n",
    "    print(\"Loading .safetensors...\")\n",
    "    state_dict = load_file(weights_path)\n",
    "else:\n",
    "    print(\"Loading .bin...\")\n",
    "    state_dict = torch.load(os.path.join(CKPT_PATH, \"pytorch_model.bin\"), map_location=\"cpu\")\n",
    "\n",
    "# Load weights into the EXISTING model object\n",
    "# strict=False is allowed because we handled the tied weights manually\n",
    "keys = model.load_state_dict(state_dict, strict=False)\n",
    "print(f\"Weights loaded. Missing keys: {keys.missing_keys}\")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training with FRESH optimizer (ignoring broken optimizer.pt)...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Saving Final Model\n",
    "We save the final fine-tuned model to a clean directory. The `trainer.save_model()` function handles saving the tokenizer and config automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "FINAL_OUTPUT_DIR = os.path.join(model_config.FINETUNED_OUTPUT_DIR, \"final_model\")\n",
    "print(f\"Saving final model to standard directory: {FINAL_OUTPUT_DIR}\")\n",
    "\n",
    "trainer.save_model(FINAL_OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(FINAL_OUTPUT_DIR)\n",
    "\n",
    "print(\"SUCCESS. Model saved safely. No manual weight patching required.\")\n",
    "print(f\"Artifacts in folder: {os.listdir(FINAL_OUTPUT_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Final Validation Check\n",
    "This is the proof of life. We reload the *saved* model from disk (to make sure the save process worked) and run a final set of translations. We verify that it loads without errors and produces coherent Arabic output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "print(\"\\n--- Final Validation: Loading from 'final_model' Dir ---\")\n",
    "\n",
    "FINAL_OUTPUT_DIR = os.path.join(model_config.FINETUNED_OUTPUT_DIR, \"final_model\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if not os.path.exists(FINAL_OUTPUT_DIR):\n",
    "    raise FileNotFoundError(f\"Model directory not found at: {FINAL_OUTPUT_DIR}\")\n",
    "\n",
    "# Define RMSNorm again for loading context\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(dim))\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return x / rms * self.scale\n",
    "\n",
    "def replace_layernorm_with_rmsnorm(module: nn.Module):\n",
    "    for name, child in list(module.named_children()):\n",
    "        if isinstance(child, nn.LayerNorm):\n",
    "            dim = child.normalized_shape[0] if isinstance(child.normalized_shape, (tuple, list)) else child.normalized_shape\n",
    "            rms = RMSNorm(dim=dim, eps=1e-6)\n",
    "            setattr(module, name, rms)\n",
    "        else:\n",
    "            replace_layernorm_with_rmsnorm(child)\n",
    "\n",
    "# Clean Memory\n",
    "if 'model' in globals(): del model\n",
    "if 'trainer' in globals(): del trainer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reload\n",
    "print(\"Loading configuration...\")\n",
    "config = AutoConfig.from_pretrained(FINAL_OUTPUT_DIR)\n",
    "new_model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "replace_layernorm_with_rmsnorm(new_model)\n",
    "\n",
    "weights_path = os.path.join(FINAL_OUTPUT_DIR, \"model.safetensors\")\n",
    "state_dict = load_file(weights_path)\n",
    "new_model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "new_model.to(device).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(FINAL_OUTPUT_DIR)\n",
    "print(\"Model ready for testing.\")\n",
    "\n",
    "test_sentences = [\n",
    "    \"Get in the car\", \n",
    "    \"I have a bad feeling\", \n",
    "    \"Open the door, please.\", \n",
    "    \"It is three o'clock in the afternoon.\",\n",
    "    \"It's raining cats and dogs.\"\n",
    "]\n",
    "\n",
    "print(\"\\n--- Validation Translations ---\")\n",
    "for i, s in enumerate(test_sentences):\n",
    "    inputs = tokenizer(s, return_tensors=\"pt\").to(device)\n",
    "    if \"token_type_ids\" in inputs: del inputs[\"token_type_ids\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = new_model.generate(**inputs, max_new_tokens=128, num_beams=5)\n",
    "    \n",
    "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    print(f\"EN:  {s}\")\n",
    "    print(f\"ARZ: {decoded}\")\n",
    "    print(\"-\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
