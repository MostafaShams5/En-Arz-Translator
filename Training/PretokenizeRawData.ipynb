{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Shams\n",
    "\n",
    "**Description:**  \n",
    "This notebook handles the pre-processing logic. I need to clean up the raw text before training the tokenizer. It splits Arabic prefixes and handles English contractions so the model understands words better. Then it saves the result to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Login to Hugging Face\n",
    "I need to log in here so I can grab my private dataset from the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# logging in to access the data\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the Data\n",
    "Pulling the dataset I uploaded earlier. I'm using a wildcard to catch all the parquet files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "from typing import List, Optional, Tuple, Set\n",
    "\n",
    "dataset_id = \"Shams03/ARZ-EN\"\n",
    "# grabbing all the parquet files\n",
    "data_files = {\"train\": \"AllDataPARQUET/*.parquet\"}\n",
    "\n",
    "dataset = load_dataset(dataset_id, data_files=data_files, split='train')\n",
    "print(\"Got the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check the Data\n",
    "Just printing a few rows to make sure I downloaded the right thing and the columns look correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data info:\")\n",
    "print(dataset)\n",
    "\n",
    "print(\"\\nColumns:\")\n",
    "print(dataset.column_names)\n",
    "\n",
    "print(\"\\nRandom samples:\")\n",
    "# check 10 random lines\n",
    "for sample in dataset.shuffle(seed=42).select(range(10)):\n",
    "    print(f\"Text: {sample['sentence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Arabic Tokenizer Logic\n",
    "Arabic is tricky because words stick together. I need to split prefixes like \"wa\" (and) or \"al\" (the) from the main word. I also need to put spaces around punctuation so the tokenizer sees them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArzTokenizer:\n",
    "    \"\"\"\n",
    "    My custom logic for splitting Arabic text.\n",
    "    It cuts off common prefixes and spaces out punctuation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # these are the sticky prefixes I want to cut off\n",
    "        self.prefixes: List[str] = ['ال', 'يا', 'لل', 'وال', 'بال']\n",
    "\n",
    "        # listing all punctuation to handle them quickly\n",
    "        punctuation_chars = '؟،؛:!.()[]{}\"\"«»–—…/\\\\|@#$%^&*+=<>~`' + \\\n",
    "                            '?,;:!.()[]{}\"\"\\'---…/\\\\|@#$%^&*+=<>~`'\n",
    "        self.punctuation_set: Set[str] = set(punctuation_chars)\n",
    "\n",
    "        # regex to catch the punctuation\n",
    "        escaped_punct = ''.join([re.escape(p) for p in self.punctuation_set])\n",
    "        self.split_pattern = re.compile(f'([{escaped_punct}])')\n",
    "\n",
    "    def _split_prefixes(self, word: str) -> List[str]:\n",
    "        # if a word starts with a prefix, cut it off\n",
    "        # but only if the remaining word is long enough\n",
    "        for prefix in self.prefixes:\n",
    "            if word.startswith(prefix) and len(word) > len(prefix):\n",
    "                return [prefix, word[len(prefix):]]\n",
    "        return [word]\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        # step 1: split by punctuation marks\n",
    "        parts = self.split_pattern.split(text)\n",
    "\n",
    "        final_tokens = []\n",
    "        for part in parts:\n",
    "            if not part:\n",
    "                continue\n",
    "\n",
    "            # step 2: if it is a symbol, just keep it\n",
    "            if part in self.punctuation_set:\n",
    "                final_tokens.append(part)\n",
    "            else:\n",
    "                # step 3: if it is words, check for prefixes\n",
    "                words = part.strip().split()\n",
    "                for word in words:\n",
    "                    final_tokens.extend(self._split_prefixes(word))\n",
    "\n",
    "        return final_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. English Tokenizer Logic\n",
    "English is easier. I mainly need to split contractions like \"don't\" into \"do\" and \"n't\" so the model learns \"not\". I also space out punctuation here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnTokenizer:\n",
    "    \"\"\"\n",
    "    Simple rules for English text.\n",
    "    Handles contractions and punctuation.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # finding things like n't, 's, 've to put a space before them\n",
    "        self.CONTRACTIONS_RE = re.compile(r\"(n't|'s|'ve|'re|'d|'ll)\\b\", re.IGNORECASE)\n",
    "\n",
    "        # finding punctuation but ignoring the apostrophe so I don't break the contractions\n",
    "        self.PUNCT_RE = re.compile(r'([!\"#$%&()*+,-./:;<=>?@\\[\\\\\\]^_`{|}~])')\n",
    "\n",
    "        # finding extra spaces\n",
    "        self.WHITESPACE_RE = re.compile(r'\\s+')\n",
    "\n",
    "    def tokenize(self, text: str) -> list[str]:\n",
    "        # step 1: space out contractions\n",
    "        text = self.CONTRACTIONS_RE.sub(r' \\1', text)\n",
    "\n",
    "        # step 2: space out symbols\n",
    "        text = self.PUNCT_RE.sub(r' \\1 ', text)\n",
    "\n",
    "        # step 3: clean up double spaces and split\n",
    "        text = self.WHITESPACE_RE.sub(' ', text).strip()\n",
    "        return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Processing the Dataset\n",
    "Now I run the whole dataset through those functions. I check the filename (`source_file`) to see if it's English or Arabic and apply the right class. I'm using `num_proc` to make it faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TokenizeArz = ArzTokenizer()\n",
    "TokenizeEN = EnTokenizer()\n",
    "\n",
    "def TokenBatches(batch):\n",
    "    \"\"\"\n",
    "    Decides which tokenizer to use based on the source file name.\n",
    "    \"\"\"\n",
    "    tokenized_output = []\n",
    "\n",
    "    # loop through the text and the filename at the same time\n",
    "    for sentence, source_file in zip(batch['sentence'], batch['source_file']):\n",
    "        \n",
    "        # if filename has ARZ, use arabic rules\n",
    "        if 'ARZ' in source_file:\n",
    "            tokens = TokenizeArz.tokenize(sentence)\n",
    "        # if filename has EN, use english rules\n",
    "        elif 'EN' in source_file:\n",
    "            tokens =TokenizeEN.tokenize(sentence)\n",
    "        else:\n",
    "            # fallback for empty or weird files\n",
    "            tokens = []\n",
    "            \n",
    "        tokenized_output.append(tokens)\n        \n    return {\"pretokenized\": tokenized_output}\n",
    "\n",
    "# running the map function\n",
    "# num_proc=4 makes it run in parallel\n",
    "MyPreTokenizedData = dataset.map(\n",
    "    TokenBatches,\n",
    "    batched=True,\n",
    "    num_proc=4, \n",
    "    batch_size=2000,\n",
    "    remove_columns=['sentence'], # dropping the old column to save space\n",
    "    desc=\"Running tokenizer...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Inspecting Results\n",
    "Checking if the column `pretokenized` was created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDone.\")\n",
    "print(\"New dataset structure:\")\n",
    "print(MyPreTokenizedData)\n",
    "\n",
    "print(\"\\nChecking a processed row:\")\n",
    "print(MyPreTokenizedData[5000000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Save to Disk\n",
    "Saving it as a Hugging Face dataset folder so I can load it quickly later without re-running this logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/kaggle/working/MyPreTokenizedData\"\n",
    "MyPreTokenizedData.save_to_disk(save_path)\n",
    "\n",
    "print(\"Saved dataset folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Save as Parquet\n",
    "Also saving as a parquet file just in case I need to move it around easily or use it in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/kaggle/working/MyPreTokenizedData.parquet\"\n",
    "\n",
    "try:\n",
    "    MyPreTokenizedData.to_parquet(save_path)\n",
    "    print(f\"Saved parquet to {save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving parquet: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Final Check\n",
    "List the directory to confirm the files are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
